{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47LqcXepWgWH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/dataset (1).csv')\n",
        "\n",
        "# Assume the DataFrame df has columns 'Example' for the text and 'Idiom' for the labels\n",
        "x = df['Example']\n",
        "y = df['Idiom']\n",
        "\n",
        "# Encode the labels to ensure they are in integer format\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "num_classes = len(np.unique(y_encoded))  # Determine the number of unique classes\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(x, y_encoded, test_size=0.4, random_state=42)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Function to train and evaluate a BERT-based classifier\n",
        "def train_and_evaluate_bert(X_train, Y_train, X_val, Y_val, X_test, Y_test, dataset_name, num_classes):\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(\"Training labels distribution:\", np.bincount(Y_train))\n",
        "    print(\"Validation labels distribution:\", np.bincount(Y_val))\n",
        "    print(\"Test labels distribution:\", np.bincount(Y_test))\n",
        "\n",
        "    # Set device to GPU if available, otherwise use CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load a pre-trained BERT model and tokenizer\n",
        "    model_name = 'bert-base-uncased'\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize the text data and convert it to PyTorch tensors\n",
        "    X_train_tokens = tokenizer(X_train.tolist(), truncation=True, padding=True, return_tensors='pt').to(device)\n",
        "    X_val_tokens = tokenizer(X_val.tolist(), truncation=True, padding=True, return_tensors='pt').to(device)\n",
        "    X_test_tokens = tokenizer(X_test.tolist(), truncation=True, padding=True, return_tensors='pt').to(device)\n",
        "\n",
        "    # Convert labels to PyTorch tensors\n",
        "    Y_train_tensor = torch.tensor(Y_train).to(device)\n",
        "    Y_val_tensor = torch.tensor(Y_val).to(device)\n",
        "    Y_test_tensor = torch.tensor(Y_test).to(device)\n",
        "\n",
        "    # Define training parameters\n",
        "    batch_size = 16\n",
        "    learning_rate = 2e-5\n",
        "    num_epochs = 3  # Adjusted for demonstration\n",
        "\n",
        "    # Create a DataLoader for training, validation, and testing data\n",
        "    train_data = torch.utils.data.TensorDataset(X_train_tokens.input_ids, X_train_tokens.attention_mask, Y_train_tensor)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "    val_data = torch.utils.data.TensorDataset(X_val_tokens.input_ids, X_val_tokens.attention_mask, Y_val_tensor)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
        "\n",
        "    test_data = torch.utils.data.TensorDataset(X_test_tokens.input_ids, X_test_tokens.attention_mask, Y_test_tensor)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    # Create an optimizer and a loss function\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predicted_labels = logits.argmax(dim=1).tolist()\n",
        "            y_pred.extend(predicted_labels)\n",
        "\n",
        "    # Convert predicted labels to NumPy array\n",
        "            y_pred = np.array(y_pred)\n",
        "\n",
        "            # Calculate and print accuracy\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(f\"Accuracy for : {accuracy:.4f}\")\n",
        "\n",
        "            # Calculate and print the macro F1 score\n",
        "            f1 = f1_score(y_test, y_pred, average='macro')\n",
        "            print(f\"F1 Score for : {f1:.4f}\")\n",
        "            return y_true, y_pred\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    Y_val_true, y_pred_val = evaluate_model(val_loader)\n",
        "    accuracy_val = accuracy_score(Y_val_true, y_pred_val)\n",
        "    f1_val = f1_score(Y_val_true, y_pred_val, average='macro')\n",
        "    print(f\"Validation Accuracy for {dataset_name}: {accuracy_val:.4f}\")\n",
        "    print(f\"Validation F1 Score for {dataset_name}: {f1_val:.4f}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    Y_test_true, y_pred_test = evaluate_model(test_loader)\n",
        "    accuracy_test = accuracy_score(Y_test_true, y_pred_test)\n",
        "    f1_test = f1_score(Y_test_true, y_pred_test, average='macro')\n",
        "    print(f\"Test Accuracy for {dataset_name}: {accuracy_test:.4f}\")\n",
        "    print(f\"Test F1 Score for {dataset_name}: {f1_test:.4f}\")\n",
        "\n",
        "# Call the function with dataset name\n",
        "train_and_evaluate_bert(X_train, Y_train, X_val, Y_val, X_test, Y_test, 'My Custom Dataset', num_classes)\n"
      ]
    }
  ]
}